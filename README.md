# How to use LangChain for AI-powered App

## 0.1 Project Requirements

### Python Virtual Environment

1. Create and activate a Python virtual environment:

```shell
python -m venv ./env
source env/Scripts/activate
```

2. Create a `requirements.txt` file and install python packages:

```shell
curl -sSL https://gist.githubusercontent.com/serranoarevalo/72d77c36dde1cc3ffec34105eb666140/raw/d18ab867affcf7e122947d2b40dbd9934dd21186/requirements.txt -o requirements.txt
```

### Manage OpenAI API Key in Environment File(`.env`)

1. Acquire OpenAI's Projects API keys([Link](https://platform.openai.com/organization/api-keys))

2. Create and add api_key into `.env` file

```shell
echo 'OPENAI_SECRET_KEY="[API_KEY]"' > .env
```

### Git Settings

1. Initalize the repository and create a `.gitignore` file:

```shell
git init .
touch .gitignore
```

2. Add the following entries to your `.gitignore` file:

- Python, JupyterNotebooks([Link](https://www.toptal.com/developers/gitignore/api/python,jupyternotebooks))
- Python Venv, Environment File

```shell
# Python Virtual Environment
env/

# Environment File
.env
```

### VSCode Settings

1. Install VScode Extensions

- `ms-python.python`
- `ms-toolsai.jupyer`

## 0.2 Understanding LangChain Usage

### 1. Configure Chat Model

```python
from langchain.chat_models import ChatOpenAI

# Create Chat Model instance
chat = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.1
)
```

- `temperate`: Determines the randomness of the model's output (range: 0 to 1).

### 2. Configure Prompt

1. Simple Text Prompt

```python
prompt="How many planets are in the solar system?"

response = chat.predict(prompt)
print(response)
```

## 0.2 Understanding LangChain Usage

### 1. Configure Chat Model

```python
from langchain.chat_models import ChatOpenAI

# Create Chat Model instance
chat = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.1
)
```

- `temperate`: Determines the randomness of the model's output (range: 0 to 1).

### 2. Configure Prompt

1. Simple Text Prompt

```python
prompt="How many planets are in the solar system?"

response = chat.predict(prompt)
print(response)
```

2. Message Object Prompt

```python
from langchain.schema import SystemMessage, HumanMessage, AIMessage

# Create a list of messages to simulate a conversation
messages = [
    SystemMessage(content="You are a geography expert. And you only reply in Italian"),
    AIMessage(content="Ciao, mi chiamo Paolo!"),
    HumanMessage(content="What is the distance between Mexico and Thailand. Also, what is your name?")
]

# Use the chat model to prdict responses based on the provided messages
response = chat.predict_messages(messages)
print(response)
```

- `SystemMessage`: Provides rules or context for how the AI should behave
- `AIMessage`: Responses generated by the AI within the conversation.
- `HumanMessage`: User's questions or comments to the AI.

3. String Prompt Template

```python
from langchain.prompts import PromptTemplate

# Create a template for the prompt with placeholders
template = PromptTemplate.from_template("What is the distance between {country_a} and {country_b}")

# Format the template with specific values for placeholders
prompt = template.format(country_a="Mexico", country_b="Thaliand")

# Use the chat model to predict the response based on the formatted prompt
response = chat.predict(prompt)
print(response)
```

- Support placeholders in `{}` which are replaced with actual values.

4. Chat Prompt Template

```python
from langchain.prompts import ChatPromptTemplate
from langchain.schema import SystemMessage, HumanMessage, AIMessage

# Create a list of messages to simulate a conversation
template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a geography expert. And you only reply in {language}"),
        ("ai", "Ciao, mi chiamo {name}!"),
        ("human", "What is the distance between {country_a} and {country_b}. Also, what is your name?"),
    ]
)

# Format the template with specific values for placeholders
prompt = template.format_messages(
    language="Greek",
    name="Socrates",
    country_a="Mexico",
    country_b="Thailand",
)

# Use the chat model to predict responses based on the formatted messages
response = chat.predict_messages(prompt)
print(response)
```

- Uses prompt template with a structured message format.

### 3. Configure OuputParser

1. OuputParser Basic Example

```python
from langchain.schema import BaseOutputParser

# Define a custom output parser
class CommaOutputParser(BaseOutputParser):

    def parse(self, text):
        items = text.strip().split(",")
        return list(map(lambda x: x.strip().lower(), items))

# Parse the content of the result
p = CommaOutputParser()
p.parse(result.content)
```

- `BaseOutputParser` is just an empty class with a parse method.
- Place own parsing logic in the `parse` method
- Ensure to convert the `ChatResult` instance into a string using the `.content` attribute before passing it to the `parse` method.

### 4. Chain LangChain Components

1. How to Create and Use a Chain

```python
# Chain template, chat model, and output parser
chain = template | chat | CommaOutputParser()

# Invoke chain with specified parameters
chain.invoke(
    {
        "max_items": 5,
        "question": "What are the pokemons?"
    }
)
```

- Chain together components like `Prompt`, `Retriever`, `ChatModel`, `Tool`, and `OutputParser` using the `|` operator.
- Use the `.invoke` method with a dictionary(`{}`) to provide actual values for the placeholders in the chain.
